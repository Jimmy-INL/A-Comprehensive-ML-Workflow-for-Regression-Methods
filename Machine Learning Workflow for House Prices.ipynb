{
  "cells": [
    {
      "metadata": {
        "_uuid": "a8f9622945156d6337ba73c481da2de7efef7384"
      },
      "cell_type": "markdown",
      "source": " ## <div align=\"center\">Machine Learning Workflow for House Prices</div>\n\nThere are plenty of **courses and tutorials** that can help you learn machine learning from scratch but here in **Kaggle**, I want to predict **House Prices**  a popular machine learning Dataset as a comprehensive workflow with python packages. \nAfter reading, you can use this workflow to solve other real problems and use it as a template to deal with machine learning problems.  \n\n<img src=\"http://s9.picofile.com/file/8338980150/House_price.png\"></img>\n<div style=\"text-align:center\">last update: <b>12/08/2018</b></div>\n\n---------------------------------------------------------------------\n\n>###### You may  be interested have a look at 10 Steps to Become a Data Scientist: \n\n1. [Leren Python](https://www.kaggle.com/mjbahmani/the-data-scientist-s-toolbox-tutorial-1)\n2. [Python Packages](https://www.kaggle.com/mjbahmani/the-data-scientist-s-toolbox-tutorial-2)\n3. [Mathematics and Linear Algebra](https://www.kaggle.com/mjbahmani/linear-algebra-for-data-scientists)\n4. [Programming &amp; Analysis Tools](https://www.kaggle.com/mjbahmani/20-ml-algorithms-15-plot-for-beginners)\n5. [Big Data](https://www.kaggle.com/mjbahmani/a-data-science-framework-for-quora)\n6. [Data visualization](https://www.kaggle.com/mjbahmani/top-5-data-visualization-libraries-tutorial)\n7. [Data Cleaning](https://www.kaggle.com/mjbahmani/machine-learning-workflow-for-house-prices)\n8. [How to solve a Problem?](https://www.kaggle.com/mjbahmani/the-data-scientist-s-toolbox-tutorial-2)\n9. [Machine Learning](https://www.kaggle.com/mjbahmani/a-comprehensive-ml-workflow-with-python)\n10. [Deep Learning](https://www.kaggle.com/mjbahmani/top-5-deep-learning-frameworks-tutorial)\n\n--------------------------------------------------------------------------------\n\nyou can Fork and Run this kernel on Github:\n> ###### [ GitHub](https://github.com/mjbahmani/Machine-Learning-Workflow-with-Python)\n\n\n-----------------------------\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated**\n \n \n -----------"
    },
    {
      "metadata": {
        "_uuid": "cda11210a88d6484112cbe2c3624225328326c6a"
      },
      "cell_type": "markdown",
      "source": " <a id=\"top\"></a> <br>\n## Notebook  Content\n1. [Introduction](#1)\n    1. [Courses](#111)\n    1. [Kaggle kernels](#112)\n    1. [Ebooks](#113)\n1. [Machine learning workflow](#2)\n1. [Problem Definition](#3)\n    1. [Problem feature](#4)\n    1. [Aim](#5)\n    1. [Variables](#6)\n    1. [ Inputs & Outputs](#7)\n        1. [Inputs ](#8)\n        1. [Outputs](#9)\n1. [Installation](#10)\n    1. [ jupyter notebook](#11)\n    1. [ kaggle kernel](#12)\n    1. [Colab notebook](#13)\n    1. [install python & packages](#14)\n    1. [Loading Packages](#15)\n1. [Exploratory data analysis](#16)\n    1. [Data Collection](#17)\n    1. [Visualization](#18)\n        1. [Scatter plot](#19)\n        1. [Box](#20)\n        1. [Histogram](#21)\n        1. [Multivariate Plots](#22)\n        1. [Violinplots](#23)\n        1. [Pair plot](#24)\n        1. [Kde plot](#25)\n        1. [Joint plot](#26)\n        1. [Andrews curves](#27)\n        1. [Heatmap](#28)\n        1. [Radviz](#29)\n    1. [Data Preprocessing](#30)\n    1. [Data Cleaning](#31)\n1. [Model Deployment](#32)\n1. [Conclusion](#53)\n1. [References](#54)"
    },
    {
      "metadata": {
        "_uuid": "750903cc2679d39058f56df6c6c040be02b748df"
      },
      "cell_type": "markdown",
      "source": " <a id=\"1\"></a> <br>\n## 1- Introduction\nThis is a **A Comprehensive ML Workflow for House Prices** data set, it is clear that everyone in this community is familiar with house prices dataset but if you need to review your information about the dataset please visit this [link](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).\n\nI have tried to help **Fans of Machine Learning**  in Kaggle how to face machine learning problems. and I think it is a great opportunity for who want to learn machine learning workflow with python **completely**.\n\nI want to covere most of the methods that are implemented for house prices until **2018**, you can start to learn and review your knowledge about ML with a simple dataset and try to learn and memorize the workflow for your journey in Data science world.\n\nBefore we get into the notebook, let me introduce some helpful resources."
    },
    {
      "metadata": {
        "_uuid": "e4801c3bbcda26c28aa7d145689d5261d5c37370"
      },
      "cell_type": "markdown",
      "source": " <a id=\"111\"></a> <br>\n## 1-1 Courses\nThere are a lot of Online courses that can help you develop your knowledge, here I have just  listed some of them:\n\n1. [Machine Learning Certification by Stanford University (Coursera)](https://www.coursera.org/learn/machine-learning/)\n\n2. [Machine Learning A-Z™: Hands-On Python & R In Data Science (Udemy)](https://www.udemy.com/machinelearning/)\n\n3. [Deep Learning Certification by Andrew Ng from deeplearning.ai (Coursera)](https://www.coursera.org/specializations/deep-learning)\n\n4. [Python for Data Science and Machine Learning Bootcamp (Udemy)](Python for Data Science and Machine Learning Bootcamp (Udemy))\n\n5. [Mathematics for Machine Learning by Imperial College London](https://www.coursera.org/specializations/mathematics-machine-learning)\n\n6. [Deep Learning A-Z™: Hands-On Artificial Neural Networks](https://www.udemy.com/deeplearning/)\n\n7. [Complete Guide to TensorFlow for Deep Learning Tutorial with Python](https://www.udemy.com/complete-guide-to-tensorflow-for-deep-learning-with-python/)\n\n8. [Data Science and Machine Learning Tutorial with Python – Hands On](https://www.udemy.com/data-science-and-machine-learning-with-python-hands-on/)\n\n9. [Machine Learning Certification by University of Washington](https://www.coursera.org/specializations/machine-learning)\n\n10. [Data Science and Machine Learning Bootcamp with R](https://www.udemy.com/data-science-and-machine-learning-bootcamp-with-r/)\n11. [Creative Applications of Deep Learning with TensorFlow](https://www.class-central.com/course/kadenze-creative-applications-of-deep-learning-with-tensorflow-6679)\n12. [Neural Networks for Machine Learning](https://www.class-central.com/mooc/398/coursera-neural-networks-for-machine-learning)\n13. [Practical Deep Learning For Coders, Part 1](https://www.class-central.com/mooc/7887/practical-deep-learning-for-coders-part-1)\n14. [Machine Learning](https://www.cs.ox.ac.uk/teaching/courses/2014-2015/ml/index.html)"
    },
    {
      "metadata": {
        "_uuid": "942f87c3d0feb55c83c4cbafa31c8c48df36901f"
      },
      "cell_type": "markdown",
      "source": " <a id=\"112\"></a> <br>\n## 1-2 Kaggle kernels\nI want to thanks **Kaggle team**  and  all of the **kernel's authors**  who develop this huge resources for Data scientists. I have learned from The work of others and I have just listed some more important kernels that inspired my work and I've used them in this kernel:\n\n1. [Comprehensive Data Exploration with python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)\n1. [A study on Regression applied to the Ames dataset](https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset)\n1. [Regularized Linear Models](https://www.kaggle.com/apapiu/regularized-linear-models)"
    },
    {
      "metadata": {
        "_uuid": "2352e37c41b407378cc25f127ebaa36c99f00801"
      },
      "cell_type": "markdown",
      "source": " <a id=\"113\"></a> <br>\n## 1-3 Ebooks\nSo you love reading , here is **10 free machine learning books**\n1. [Probability and Statistics for Programmers](http://www.greenteapress.com/thinkstats/)\n2. [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/091117.pdf)\n2. [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\n2. [Understanding Machine Learning](http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html)\n2. [A Programmer’s Guide to Data Mining](http://guidetodatamining.com/)\n2. [Mining of Massive Datasets](http://infolab.stanford.edu/~ullman/mmds/book.pdf)\n2. [A Brief Introduction to Neural Networks](http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-2col-dkrieselcom.pdf)\n2. [Deep Learning](http://www.deeplearningbook.org/)\n2. [Natural Language Processing with Python](https://www.researchgate.net/publication/220691633_Natural_Language_Processing_with_Python)\n2. [Machine Learning Yearning](http://www.mlyearning.org/)\n\n\nI am open to your feedback for improving this **kernel**\n\n[go to top](#top)"
    },
    {
      "metadata": {
        "_uuid": "e11b73b618b0f6e4335520ef80267c6d577d1ba5"
      },
      "cell_type": "markdown",
      "source": "<a id=\"2\"></a> <br>\n## 2- Machine Learning Workflow\nIf you have already read some [machine learning books](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/tree/master/Ebooks). You have noticed that there are different ways to stream data into machine learning.\n\nMost of these books share the following steps:\n1. Define Problem\n1. Specify Inputs & Outputs\n1. Exploratory data analysis\n1. Data Collection\n1. Data Preprocessing\n1. Data Cleaning\n1. Visualization\n1. Model Design, Training, and Offline Evaluation\n1. Model Deployment, Online Evaluation, and Monitoring\n1. Model Maintenance, Diagnosis, and Retraining\n\nOf course, the same solution can not be provided for all problems, so the best way is to create a **general framework** and adapt it to new problem.\n\n**You can see my workflow in the below image** :\n\n <img src=\"http://s8.picofile.com/file/8344100018/workflow3.png\" />\n\n**Data Science has so many techniques and procedures that can confuse anyone.**\n"
    },
    {
      "metadata": {
        "_uuid": "8f90140a4bce24083d97e3650243145d0d4dcbeb"
      },
      "cell_type": "markdown",
      "source": "## 2-2 Real world Application Vs Competitions\nWe all know that there are differences between real world problem and competition problem. The following figure that is taken from one of the courses in coursera, has partly made this comparison \n\n<img src=\"http://s9.picofile.com/file/8339956300/reallife.png\" height=\"600\" width=\"500\" />\n\nAs you can see, there are a lot more steps to solve  in real problems."
    },
    {
      "metadata": {
        "_uuid": "600be852c0d28e7c0c5ebb718904ab15a536342c"
      },
      "cell_type": "markdown",
      "source": "<a id=\"3\"></a> <br>\n## 3- Problem Definition\nI think one of the important things when you start a new machine learning project is defining your problem.that means you should understand business problem.( **Problem Formalization**).\n\nProblem definition has four steps that have illustrated in the picture below:\n<img src=\"http://s8.picofile.com/file/8344103134/Problem_Definition2.png\" height=\"600\" width=\"500\" >\n<a id=\"4\"></a> <br>\n### 3-1 Problem Feature\nWe will use the house prices data set. This dataset contains information about house prices and the target value is:\n\n1. SalePrice\n\n**Why am I  using House price dataset:**\n\n1. This is a good project because it is so well understood.\n1. Attributes are numeric and categurical so you have to figure out how to load and handle data.\n1. It is a Regression problem, allowing you to practice with perhaps an easier type of supervised learning algorithm.\n1. This is a perfect competition for data science students who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition. \n1. Creative feature engineering ."
    },
    {
      "metadata": {
        "_uuid": "2a3a73558906ee81dd633c3ccb95b3aa2366293b"
      },
      "cell_type": "markdown",
      "source": "#### 3-1-1 Metric\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n<img src='https://i.stack.imgur.com/eG03B.png' height= 200 width=350>\n<a id=\"5\"></a> <br>\n### 3-2 Aim\nIt is our job to predict the sales price for each house. for each Id in the test set, you must predict the value of the **SalePrice** variable. \n\n<a id=\"6\"></a> <br>\n### 3-3 Variables\nThe variables are :\n* SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n* MSSubClass: The building class\n* MSZoning: The general zoning classification\n* LotFrontage: Linear feet of street connected to property\n* LotArea: Lot size in square feet\n* Street: Type of road access\n* Alley: Type of alley access\n* LotShape: General shape of property\n* LandContour: Flatness of the property\n* Utilities: Type of utilities available\n* LotConfig: Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n* YearRemodAdd: Remodel date\n* RoofStyle: Type of roof\n* RoofMatl: Roof material\n* Exterior1st: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* MasVnrType: Masonry veneer type\n* MasVnrArea: Masonry veneer area in square feet\n* ExterQual: Exterior material quality\n* ExterCond: Present condition of the material on the exterior\n* Foundation: Type of foundation\n* BsmtQual: Height of the basement\n* BsmtCond: General condition of the basement\n* BsmtExposure: Walkout or garden level basement walls\n* BsmtFinType1: Quality of basement finished area\n* BsmtFinSF1: Type 1 finished square feet\n* BsmtFinType2: Quality of second finished area (if present)\n* BsmtFinSF2: Type 2 finished square feet\n* BsmtUnfSF: Unfinished square feet of basement area\n* TotalBsmtSF: Total square feet of basement area\n* Heating: Type of heating\n* HeatingQC: Heating quality and condition\n* CentralAir: Central air conditioning\n* Electrical: Electrical system\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n*  LowQualFinSF: Low quality finished square feet (all floors)\n* GrLivArea: Above grade (ground) living area square feet\n* BsmtFullBath: Basement full bathrooms\n* BsmtHalfBath: Basement half bathrooms\n* FullBath: Full bathrooms above grade\n* HalfBath: Half baths above grade\n* Bedroom: Number of bedrooms above basement level\n* Kitchen: Number of kitchens\n* KitchenQual: Kitchen quality\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* Functional: Home functionality rating\n* Fireplaces: Number of fireplaces\n* FireplaceQu: Fireplace quality\n* GarageType: Garage location\n* GarageYrBlt: Year garage was built\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n* PavedDrive: Paved driveway\n* WoodDeckSF: Wood deck area in square feet\n* OpenPorchSF: Open porch area in square feet\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n* PoolArea: Pool area in square feet\n* PoolQC: Pool quality\n* Fence: Fence quality\n* MiscFeature: Miscellaneous feature not covered in other categories\n* MiscVal: Value of miscellaneous feature\n* MoSold: Month Sold\n* YrSold: Year Sold\n* SaleType: Type of sale\n* SaleCondition: Condition of sale"
    },
    {
      "metadata": {
        "_uuid": "8bb4dfebb521f83543e1d45db3559216dad8f6fb"
      },
      "cell_type": "markdown",
      "source": "<a id=\"7\"></a> <br>\n## 4- Inputs & Outputs\nFor every machine learning problem, you should ask yourself, what are inputs and outputs for the model?\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/bc/Input-Output.JPG\" height=300 width=300 ></img>\n\n<a id=\"8\"></a> <br>\n### 4-1 Inputs\n* train.csv - the training set\n* test.csv - the test set\n\n<a id=\"9\"></a> <br>\n\n### 4-2 Outputs\n* sale prices for every record in test.csv"
    },
    {
      "metadata": {
        "_uuid": "89ee0cda57822cd4102eadf8992c5bfe1964d557"
      },
      "cell_type": "markdown",
      "source": "<a id=\"10\"></a> <br>\n## 5-Installation\n#### Windows:\n* Anaconda (from https://www.continuum.io) is a free Python distribution for SciPy stack. It is also available for Linux and Mac.\n* Canopy (https://www.enthought.com/products/canopy/) is available as free as well as commercial distribution with full SciPy stack for Windows, Linux and Mac.\n* Python (x,y) is a free Python distribution with SciPy stack and Spyder IDE for Windows OS. (Downloadable from http://python-xy.github.io/)"
    },
    {
      "metadata": {
        "_uuid": "e147fe588ffe4da2395c998bcb03da74fcb8f9ce"
      },
      "cell_type": "markdown",
      "source": "#### Linux:\nPackage managers of respective Linux distributions are used to install one or more packages in SciPy stack.\n\nFor Ubuntu Users:\nsudo apt-get install python-numpy python-scipy python-matplotlibipythonipythonnotebook\npython-pandas python-sympy python-nose"
    },
    {
      "metadata": {
        "_uuid": "c1793fb141d3338bbc4300874be6ffa5cb1a9139"
      },
      "cell_type": "markdown",
      "source": "<a id=\"11\"></a> <br>\n## 5-1 Jupyter notebook\nI strongly recommend installing **Python** and **Jupyter** using the **[Anaconda Distribution](https://www.anaconda.com/download/)**, which includes Python, the Jupyter Notebook, and other commonly used packages for scientific computing and data science.\n<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/jupyter-768x382.png\" height=400 width=400></img>\nFirst, download Anaconda. We recommend downloading Anaconda’s latest Python 3 version.\n\nSecond, install the version of Anaconda which you downloaded, following the instructions on the download page.\n\nCongratulations, you have installed Jupyter Notebook! To run the notebook, run the following command at the Terminal (Mac/Linux) or Command Prompt (Windows):"
    },
    {
      "metadata": {
        "_uuid": "abbd1757dde9805758a2cec47a186e31dbc29822"
      },
      "cell_type": "markdown",
      "source": "> jupyter notebook\n> "
    },
    {
      "metadata": {
        "_uuid": "8a70c253d5afa93f07a7a7e048dbb2d7812c8d10"
      },
      "cell_type": "markdown",
      "source": "<a id=\"12\"></a> <br>\n## 5-2 Kaggle Kernel\nKaggle kernel is an environment just like you use jupyter notebook, it's an **extension** of the where in you are able to carry out all the functions of jupyter notebooks plus it has some added tools like forking et al."
    },
    {
      "metadata": {
        "_uuid": "237bbe4e4509c9491ce165e3599c432b979d7b90"
      },
      "cell_type": "markdown",
      "source": "<a id=\"13\"></a> <br>\n## 5-3 Colab notebook\n**Colaboratory** is a research tool for machine learning education and research. It’s a Jupyter notebook environment that requires no setup to use.\n### 5-3-1 What browsers are supported?\nColaboratory works with most major browsers, and is most thoroughly tested with desktop versions of Chrome and Firefox.\n### 5-3-2 Is it free to use?\nYes. Colaboratory is a research project that is free to use.\n### 5-3-3 What is the difference between Jupyter and Colaboratory?\nJupyter is the open source project on which Colaboratory is based. Colaboratory allows you to use and share Jupyter notebooks with others without having to download, install, or run anything on your own computer other than a browser."
    },
    {
      "metadata": {
        "_uuid": "fbedcae8843986c2139f18dad4b5f313e6535ac5"
      },
      "cell_type": "markdown",
      "source": "<a id=\"15\"></a> <br>\n## 5-5 Loading Packages\nIn this kernel we are using the following packages:"
    },
    {
      "metadata": {
        "_uuid": "61f49281fdd8592b44c0867225f57e6fce36342c"
      },
      "cell_type": "markdown",
      "source": " <img src=\"http://s8.picofile.com/file/8338227868/packages.png\">"
    },
    {
      "metadata": {
        "_uuid": "35dd399b3f532eef15308fa129b1c02cbc81c51e"
      },
      "cell_type": "markdown",
      "source": "### 5-5-1 Import"
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nimport scipy.stats as stats\nimport lightgbm as lgb\nimport seaborn as sns\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport json\nimport sys\nimport csv\nimport os",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9298a13104e2d408f1fef3948b956d12c3bfc249"
      },
      "cell_type": "markdown",
      "source": "### 5-5-2 Version"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0eb53d55656c33da7c8e75a78e7cca4a9a6d16f5",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "print('matplotlib: {}'.format(matplotlib.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))\nprint('scipy: {}'.format(scipy.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8a83acbcb529a6187b772b6d5f7e7313b31f3d2b"
      },
      "cell_type": "markdown",
      "source": "### 5-5-3 Setup\n\nA few tiny adjustments for better **code readability**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "158d655ea9d7fd41fb8ad9936b1ca0ad4b4c8464",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "pd.set_option('display.float_format', lambda x: '%.3f' % x)\nsns.set(style='white', context='notebook', palette='deep')\nwarnings.filterwarnings('ignore')\nsns.set_style('white')\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "04ff1a533119d589baee777c21194a951168b0c7"
      },
      "cell_type": "markdown",
      "source": "<a id=\"16\"></a> <br>\n## 6- Exploratory Data Analysis(EDA)\n In this section, you'll learn how to use graphical and numerical techniques to begin uncovering the structure of your data. \n \n1. Which variables suggest interesting relationships?\n1. Which observations are unusual?\n\nBy the end of the section, you'll be able to answer these questions and more, while generating graphics that are both insightful and beautiful.  then We will review analytical and statistical operations:\n\n1. Data Collection\n1. Visualization\n1. Data Cleaning\n1. Data Preprocessing\n<img src=\"http://s9.picofile.com/file/8338476134/EDA.png\">"
    },
    {
      "metadata": {
        "_uuid": "cedecea930b278f86292367cc28d2996a235a169"
      },
      "cell_type": "markdown",
      "source": "<a id=\"17\"></a> <br>\n## 6-1 Data Collection\n**Data collection** is the process of gathering and measuring data, information or any variables of interest in a standardized and established manner that enables the collector to answer or test hypothesis and evaluate outcomes of the particular collection.[techopedia]\n\n\n> **<< Note >>**\n\n> **The rows being the samples and the columns being attributes**\n"
    },
    {
      "metadata": {
        "_uuid": "9269ae851b744856bce56840637030a16a5877e1",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# import Dataset to play with it\ntrain = pd.read_csv('../input/train.csv')\ntest= pd.read_csv('../input/test.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "678050c147b1335649fe6b2f66fc8509935a7c77",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "y_id=test['Id'].copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "06138dc1aa7f49b304e1b789a31052b7bfda73a9"
      },
      "cell_type": "markdown",
      "source": "The **concat** function does all of the heavy lifting of performing concatenation operations along an axis. Let us create all_data."
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "2eabf8d3386083189cb10f341e73be9128c4283a"
      },
      "cell_type": "code",
      "source": "\nall_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                      test.loc[:,'MSSubClass':'SaleCondition']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "58ed9c838069f54de5cf90b20a774c3e236149b3"
      },
      "cell_type": "markdown",
      "source": "**<< Note 1 >>**\n\n1. Each row is an observation (also known as : sample, example, instance, record)\n1. Each column is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate)"
    },
    {
      "metadata": {
        "_uuid": "7b5fd1034cd591ebd29fba1c77d342ec2b408d13"
      },
      "cell_type": "markdown",
      "source": "After loading the data via **pandas**, we should checkout what the content is, description and via the following:"
    },
    {
      "metadata": {
        "_uuid": "edd043f8feb76cfe51b79785302ca4936ceb7b51",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "type(train),type(test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "581b90e6a869c3793472c7edd59091d6d6342fb2"
      },
      "cell_type": "markdown",
      "source": "\n## 6-1-1 Statistical Summary\n1- Dimensions of the dataset.\n\n2- Peek at the data itself.\n\n3- Statistical summary of all attributes.\n\n4- Breakdown of the data by the class variable.[7]\n\nDon’t worry, each look at the data is **one command**. These are useful commands that you can use again and again on future projects.\n"
    },
    {
      "metadata": {
        "_uuid": "4b45251be7be77333051fe738639104ae1005fa5",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# shape\nprint(train.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "52ab14a7403315e4b9181c5578cd53ffe8cefbfd"
      },
      "cell_type": "markdown",
      "source": "Train has one column more than test why?   (yes ==>> **target value**)"
    },
    {
      "metadata": {
        "_uuid": "145f02fa22f216885a59cddee46dc08550426fa9",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# shape\nprint(test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c64e9d3e0bf394fb833de94a0fc5c34f69fce24c",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "#columns*rows\ntrain.size,test.size",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "277e1998627d6a3ddeff4e913a6b8c3dc81dec96"
      },
      "cell_type": "markdown",
      "source": "\nWe can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property.\n\n**You should see 1460 instances and  81  attributes for train and 1459 instances and  80 attributes for test**"
    },
    {
      "metadata": {
        "_uuid": "95ee5e18f97bc410df1e54ac74e32cdff2b30755"
      },
      "cell_type": "markdown",
      "source": "for getting some information about the dataset you can use **info()** command"
    },
    {
      "metadata": {
        "_uuid": "ca840f02925751186f87e402fcb5f637ab1ab8a0",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "print(train.info())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b8c23c40749ebc193b55facb2267b488c3a5c7e5"
      },
      "cell_type": "markdown",
      "source": "**if you want see the type of data and unique value of it you use following script**"
    },
    {
      "metadata": {
        "_uuid": "4b90d165a007106ae99809ad28edd75bd8153dd8",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "train['Fence'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8accfbddf2228274ad412c3ad3be72b4107d6f6c",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "train[\"Fence\"].value_counts()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ae08b544a8d4202c7d0a47ec83d685e81c91a66d"
      },
      "cell_type": "markdown",
      "source": "**to check the first 5 rows of the data set, we can use head(5).**"
    },
    {
      "metadata": {
        "_uuid": "5899889553c3416b27e93efceddb106eb71f5156",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "train.head(5) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1150b6ac3d82562aefd5c64f9f01accee5eace4d"
      },
      "cell_type": "markdown",
      "source": "**to check out last 5 row of the data set, we use tail() function**"
    },
    {
      "metadata": {
        "_uuid": "79339442ff1f53ae1054d794337b9541295d3305",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "train.tail() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2c288c3dc8656a872a8529368812546e434d3a22"
      },
      "cell_type": "markdown",
      "source": "to pop up 5 random rows from the data set, we can use **sample(5)**  function"
    },
    {
      "metadata": {
        "_uuid": "09eb18d1fcf4a2b73ba2f5ddce99dfa521681140",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "train.sample(5) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c8a1cc36348c68fb98d6cb28aa9919fc5f2892f3"
      },
      "cell_type": "markdown",
      "source": "To give a **statistical summary** about the dataset, we can use **describe()"
    },
    {
      "metadata": {
        "_uuid": "3f7211e96627b9a81c5b620a9ba61446f7719ea3",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "train.describe() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "031d16ca235837e889734635ecff193be64b27a4"
      },
      "cell_type": "markdown",
      "source": "To check out how many null info are on the dataset, we can use **isnull().sum()"
    },
    {
      "metadata": {
        "_uuid": "8807b632269e2fa734ad26e8513199400fc09a83",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "train.isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "446e6162e16325213047ff31454813455668b574",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "train.groupby('SaleType').count()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c2f1eaf0b6dfdc7cc4dace04614e99ed56425d00"
      },
      "cell_type": "markdown",
      "source": "to print dataset **columns**, we can use columns atribute"
    },
    {
      "metadata": {
        "_uuid": "909d61b33ec06249d0842e6115597bbacf21163f",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "train.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f19c27cec4367f8b176d2609f12d22afbd7e1e0d",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "type((train.columns))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "22bc5d81c18275ee1fb082c0adbb7a65bdbec4cc"
      },
      "cell_type": "markdown",
      "source": "**<< Note 2 >>**\nin pandas's data frame you can perform some query such as \"where\""
    },
    {
      "metadata": {
        "_uuid": "8b545ff7e8367c5ab9c1db710f70b6936ac8422c",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "train[train['SalePrice']>700000]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "85d23b77d7fb1bb3e000535dc2475aca5862f242"
      },
      "cell_type": "markdown",
      "source": "## 6-1-2 Target Value Analysis\nAs you know **SalePrice** is our target value that we should predict it then now we take a look at it"
    },
    {
      "metadata": {
        "_uuid": "2d891ad744dec398f3920ce0fd9458c00671ed7b",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "train['SalePrice'].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "09f6d29aa3a63f1d0d3033e7ef6b544fd8035889"
      },
      "cell_type": "markdown",
      "source": "Flexibly plot a univariate distribution of observations.\n\n"
    },
    {
      "metadata": {
        "_uuid": "c7dceaddee057bb78a930c519221d3ad43eed360",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "sns.set(rc={'figure.figsize':(9,7)})\nsns.distplot(train['SalePrice']);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b8c91bd9590ebb02f166c6d07a401faa8d263261"
      },
      "cell_type": "markdown",
      "source": "### 6-1-3 Skewness vs Kurtosis\n1. Skewness\n    1. It is the degree of distortion from the symmetrical bell curve or the normal distribution. It measures the lack of symmetry in data distribution. It differentiates extreme values in one versus the other tail. A symmetrical distribution will have a skewness of 0.\n<img src='https://www.parsmodir.com/db/statistic/skewness.png'>\n1. Kurtosis\n    1. Kurtosis is all about the tails of the distribution — not the peakedness or flatness. It is used to describe the extreme values in one versus the other tail. It is actually the measure of outliers present in the distribution.\n"
    },
    {
      "metadata": {
        "_uuid": "a4fe40f3d3f57bf1369c7b9847c5c05ec704c0b6",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "055772bd170aa8018aabd85106b76675802c33b3"
      },
      "cell_type": "markdown",
      "source": "<a id=\"18\"></a> <br>\n## 6-2 Visualization\n**Data visualization**  is the presentation of data in a pictorial or graphical format. It enables decision makers to see analytics presented visually, so they can grasp difficult concepts or identify new patterns.\n\nWith interactive visualization, you can take the concept a step further by using technology to drill down into charts and graphs for more detail, interactively changing what data you see and how it’s processed.[SAS]\n\n In this section I show you  **11 plots** with **matplotlib** and **seaborn** that is listed in the blew picture:\n <img src=\"http://s8.picofile.com/file/8338475500/visualization.jpg\" />\n"
    },
    {
      "metadata": {
        "_uuid": "b0014a7a52e714996bc443981c853095926d20e5"
      },
      "cell_type": "markdown",
      "source": "<a id=\"19\"></a> <br>\n### 6-2-1 Scatter plot\n\nScatter plot Purpose To identify the type of relationship (if any) between two quantitative variables\n\n\n"
    },
    {
      "metadata": {
        "_uuid": "af099546eed64ebc796403d4139cb4c977c27b03",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# Modify the graph above by assigning each species an individual color.\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\ng=sns.FacetGrid(train[columns], hue=\"OverallQual\", size=5) \\\n   .map(plt.scatter, \"OverallQual\", \"SalePrice\") \\\n   .add_legend()\ng=g.map(plt.scatter, \"OverallQual\", \"SalePrice\",edgecolor=\"w\").add_legend();\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d1c7b62b5f8cba427bca13049256365141655372"
      },
      "cell_type": "markdown",
      "source": "<a id=\"20\"></a> <br>\n### 6-2-2 Box\nIn descriptive statistics, a **box plot** or boxplot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram.[wikipedia]"
    },
    {
      "metadata": {
        "_uuid": "0655e20f31a582f861d391308a088778cd7eaae9",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "train[columns].plot(y='SalePrice',x='OverallQual',kind='box')\nplt.figure()\n#This gives us a much clearer idea of the distribution of the input attributes:\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e3ad1693281b8858f5fd21827bb5c6015fa684c7",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "sns.boxplot(x=\"SalePrice\", y=\"OverallQual\", data=train[columns] )\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ad8774475841cac4bb2559b64b70252b360a9aa8",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "data = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(12, 8))\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=data)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f7f6426fd44bcd77d35a5fdbc8c4fc4f18d991ad",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# To plot the species data using a box plot:\n\nsns.boxplot(x=\"OverallQual\", y=\"SalePrice\", data=train[columns] )\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7b193e4aa7e6fb337d3f65c334849094addd097a",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# Use Seaborn's striplot to add data points on top of the box plot \n# Insert jitter=True so that the data points remain scattered and not piled into a verticle line.\n# Assign ax to each axis, so that each plot is ontop of the previous axis. \n\nax= sns.boxplot(x=\"OverallQual\", y=\"SalePrice\", data=train[columns])\nax= sns.stripplot(x=\"OverallQual\", y=\"SalePrice\", data=train[columns], jitter=True, edgecolor=\"gray\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "743a92c3c2fff1a1f99845518247f7971ad18b7c"
      },
      "cell_type": "markdown",
      "source": "<a id=\"21\"></a> <br>\n### 6-2-3 Histogram\nWe can also create a **histogram** of each input variable to get an idea of the distribution.\n\n"
    },
    {
      "metadata": {
        "_uuid": "5da0520ed3e738ee8814b2d91843ed4acec2b6e6",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# histograms\ntrain.hist(figsize=(15,20))\nplt.figure()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4757fe5eb0f6348eca563a95c7f949a8cfd0773c",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "mini_train=train[columns]\nf,ax=plt.subplots(1,2,figsize=(20,10))\nmini_train[mini_train['SalePrice']>100000].GarageArea.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')\nax[0].set_title('SalePrice>100000')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\nmini_train[mini_train['SalePrice']<100000].GarageArea.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')\nax[1].set_title('SalePrice<100000')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9fe54fd42ea5055cca47338078553c1111cded65",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": " \nmini_train[['SalePrice','OverallQual']].groupby(['OverallQual']).mean().plot.bar()\n ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e33e2dd71bafabc2d78c6ee250d5beeefabe841",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "train['OverallQual'].value_counts().plot(kind=\"bar\");",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b4e3de19781686010c6038f0e3076eb678398169"
      },
      "cell_type": "markdown",
      "source": "It looks like perhaps two of the input variables have a Gaussian distribution. This is useful to note as we can use algorithms that can exploit this assumption.\n\n"
    },
    {
      "metadata": {
        "_uuid": "3bbff56707484f88625eb8ef309b712ba03f939e"
      },
      "cell_type": "markdown",
      "source": "<a id=\"22\"></a> <br>\n### 6-2-4 Multivariate Plots\nNow we can look at the interactions between the variables.\n\nFirst, let’s look at scatterplots of all pairs of attributes. This can be helpful to spot structured relationships between input variables."
    },
    {
      "metadata": {
        "_uuid": "eb4e5d117e4ef40d7668632f42130206a5537bd0",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "\n# scatter plot matrix\npd.plotting.scatter_matrix(train[columns],figsize=(10,10))\nplt.figure()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "de7fea7986071fafbe0b93933e3beda445cbe373"
      },
      "cell_type": "markdown",
      "source": "Note the diagonal grouping of some pairs of attributes. This suggests a high correlation and a predictable relationship."
    },
    {
      "metadata": {
        "_uuid": "e0f696ec021ec99c1058a62e22c8b73082fe6fa7"
      },
      "cell_type": "markdown",
      "source": "<a id=\"23\"></a> <br>\n### 6-2-5 violinplots"
    },
    {
      "metadata": {
        "_uuid": "e352d2f8340609adf4bf6718b1d2ecee0fa730b5",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# violinplots on petal-length for each species\nsns.violinplot(data=train,x=\"Functional\", y=\"SalePrice\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0ed35bceb87051e56316d35a630334518e8b8c64"
      },
      "cell_type": "markdown",
      "source": "<a id=\"24\"></a> <br>\n### 6-2-6 pairplot"
    },
    {
      "metadata": {
        "_uuid": "b80350add6f9a742f10bffc4b497562f8bebea95",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# Using seaborn pairplot to see the bivariate relation between each pair of features\nsns.set()\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.pairplot(train[columns],size = 2 ,kind ='scatter')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2544d3c2dd34a360d295019d8cb597c7ef8f66bc"
      },
      "cell_type": "markdown",
      "source": "<a id=\"25\"></a> <br>\n###  6-2-7 kdeplot"
    },
    {
      "metadata": {
        "_uuid": "1d07222b89303b386e9e824d52cc73c045667f25",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# seaborn's kdeplot, plots univariate or bivariate density estimates.\n#Size can be changed by tweeking the value used\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.FacetGrid(train[columns], hue=\"OverallQual\", size=5).map(sns.kdeplot, \"YearBuilt\").add_legend()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "560d8e8f17bacefaf8c3855a9648f26b82fdee9b"
      },
      "cell_type": "markdown",
      "source": "<a id=\"26\"></a> <br>\n### 6-2-8 jointplot"
    },
    {
      "metadata": {
        "_uuid": "4adb4da16ea61e0f1a12bc9925dfbbaaa81e0360",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# Use seaborn's jointplot to make a hexagonal bin plot\n#Set desired size and ratio and choose a color.\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.jointplot(x=\"OverallQual\", y=\"SalePrice\", data=train[columns], size=10,ratio=10, kind='hex',color='green')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3768e31e990bfe4c2ff7b45087fbba85e0560d00"
      },
      "cell_type": "markdown",
      "source": "<a id=\"27\"></a> <br>\n###  6-2-9 andrews_curves"
    },
    {
      "metadata": {
        "_uuid": "937b6856d109001db14a3ac99568df45efbe1070",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "#In Pandas use Andrews Curves to plot and visualize data structure.\n#Each multivariate observation is transformed into a curve and represents the coefficients of a Fourier series.\n#This useful for detecting outliers in times series data.\n#Use colormap to change the color of the curves\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nfrom pandas.tools.plotting import andrews_curves\nandrews_curves(train[columns], \"OverallQual\",colormap='rainbow')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "263eaa9d2bfad0f8c68b6e8e874bdc11a6e802ac",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# we will use seaborn jointplot shows bivariate scatterplots and univariate histograms with Kernel density \n# estimation in the same figure\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.jointplot(x=\"SalePrice\", y=\"YearBuilt\", data=train[columns], size=6, kind='kde', color='#800000', space=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8e73333289d17dd648b7b2112d7fe3fe7ea444d0"
      },
      "cell_type": "markdown",
      "source": "<a id=\"28\"></a> <br>\n### 6-2-10 Heatmap"
    },
    {
      "metadata": {
        "_uuid": "3100955ca9dc61ac7d435e9c064d10d06f26afa7",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(7,4)) \ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.heatmap(train[columns].corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b81dbdd5dd8cb92a86b1f7114ffb6f088458a527"
      },
      "cell_type": "markdown",
      "source": "<a id=\"29\"></a> <br>\n### 6-2-11 radviz"
    },
    {
      "metadata": {
        "_uuid": "33fed3027d7242227d612a84bbb42b012356091b",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from pandas.tools.plotting import radviz\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nradviz(train[columns], \"OverallQual\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "03279670a5b71b7189f50571f1aaaaa45cca9881"
      },
      "cell_type": "markdown",
      "source": "## 6-2-12 Factorplot"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "68b7bc84f1966b1b1cf4e3dbb335c98153ea9398",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "sns.factorplot('OverallQual','SalePrice',hue='Functional',data=train)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "54ecb0d0a7985d86fac77ee4171f13fedbffa0a2"
      },
      "cell_type": "markdown",
      "source": "<a id=\"30\"></a> <br>\n## 6-3 Data Preprocessing\n**Data preprocessing** refers to the transformations applied to our data before feeding it to the algorithm.\n \nData Preprocessing is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.\nthere are plenty of steps for data preprocessing and we just listed some of them :\n1. removing Target column (id)\n1. Sampling (without replacement)\n1. Making part of iris unbalanced and balancing (with undersampling and SMOTE)\n1. Introducing missing values and treating them (replacing by average values)\n1. Noise filtering\n1. Data discretization\n1. Normalization and standardization\n1. PCA analysis\n1. Feature selection (filter, embedded, wrapper)\n"
    },
    {
      "metadata": {
        "_uuid": "1d8efac2b4fe6c3eda41bc93e4f3ff53381034e5"
      },
      "cell_type": "markdown",
      "source": "## 6-3-1 Noise filtering (Outliers)\nAn outlier is a data point that is distant from other similar points. Further simplifying an outlier is an observation that lies on abnormal observation amongst the normal observations in a sample set of population.\n<img src='https://cdn-images-1.medium.com/max/800/1*TbUF_HTQ6jOhO8EoPnmekQ.jpeg' height=400 width=400>\nIn statistics, an outlier is an observation point that is distant from other observations.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "262e8777242c798eb2eed85935dac05a81586260",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# Looking for outliers, as indicated in https://ww2.amstat.org/publications/jse/v19n3/decock.pdf\nplt.scatter(train.GrLivArea, train.SalePrice, c = \"blue\", marker = \"s\")\nplt.title(\"Looking for outliers\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()\n\ntrain = train[train.GrLivArea < 4000]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "32b02ba7e2e34bc732eab6d1b21033bb1205b6a6"
      },
      "cell_type": "markdown",
      "source": "2 extreme outliers on the bottom right"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e186d46c36a020801ab5b6ad4ca9600aa1073f85",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "#deleting points\ntrain.sort_values(by = 'GrLivArea', ascending = False)[:2]\ntrain = train.drop(train[train['Id'] == 1299].index)\ntrain = train.drop(train[train['Id'] == 524].index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c8c82494f314ec22b7bba34e91a0602095a65d4",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1acb24ba9286dab9f63f83032aaa0d4d9ec3666f",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "all_data = pd.get_dummies(all_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1513e3c446c7e3227d4b421632b1816909122b5c",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# Log transform the target for official scoring\n#The key point is to to log_transform the numeric variables since most of them are skewed.\ntrain.SalePrice = np.log1p(train.SalePrice)\ny = train.SalePrice",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "975d56d86e8e49b5836f9664856a4c37b6a9e61b"
      },
      "cell_type": "markdown",
      "source": "Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "954c46bfaa026de09b864082f3052a0d95e5cf30",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "plt.scatter(train.GrLivArea, train.SalePrice, c = \"blue\", marker = \"s\")\nplt.title(\"Looking for outliers\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ba90a0973c207b683b2cb5f871238cab674c74da"
      },
      "cell_type": "markdown",
      "source": "<a id=\"31\"></a> <br>\n## 6-4 Data Cleaning\nWhen dealing with real-world data, dirty data is the norm rather than the exception. We continuously need to predict correct values, impute missing ones, and find links between various data artefacts such as schemas and records. We need to stop treating data cleaning as a piecemeal exercise (resolving different types of errors in isolation), and instead leverage all signals and resources (such as constraints, available statistics, and dictionaries) to accurately predict corrective actions."
    },
    {
      "metadata": {
        "_uuid": "067f2cee5b48555d0ede873e28445531efd55c10"
      },
      "cell_type": "markdown",
      "source": "## 6-4-1 Handle missing values\nFirstly, understand that there is NO good way to deal with missing data\n<img src='https://cdn-images-1.medium.com/max/800/1*_RA3mCS30Pr0vUxbp25Yxw.png'>"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "568d77bd198855749b3d0c235873bf2ef0f65c28",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "#filling NA's with the mean of the column:\nall_data = all_data.fillna(all_data.mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7650b640ca94172c6f202806109b2213c99a5042"
      },
      "cell_type": "markdown",
      "source": "<a id=\"32\"></a> <br>\n## 7- Model Deployment\nIn this section have been applied plenty of  ** learning algorithms** that play an important rule in your experiences and improve your knowledge in case of ML technique.\n\n> **<< Note 3 >>** : The results shown here may be slightly different for your analysis because, for example, the neural network algorithms use random number generators for fixing the initial value of the weights (starting points) of the neural networks, which often result in obtaining slightly different (local minima) solutions each time you run the analysis. Also note that changing the seed for the random number generator used to create the train, test, and validation samples can change your results.\n\n[go to top](#top)"
    },
    {
      "metadata": {
        "_uuid": "c4c099ed0f9a7a38406e60509ee0e9bdf54fa41d"
      },
      "cell_type": "markdown",
      "source": "## 7-1 Families of ML algorithms\nThere are several categories for machine learning algorithms, below are some of these categories:\n* Linear\n    * Linear Regression\n    * Logistic Regression\n    * Support Vector Machines\n* Tree-Based\n    * Decision Tree\n    * Random Forest\n    * GBDT\n* KNN\n* Neural Networks\n\n-----------------------------\nAnd if we  want to categorize ML algorithms with the type of learning, there are below type:\n* Classification\n\n    * k-Nearest \tNeighbors\n    * LinearRegression\n    * SVM\n    * DT \n    * NN\n    \n* clustering\n\n    * K-means\n    * HCA\n    * Expectation Maximization\n    \n* Visualization \tand\tdimensionality \treduction:\n\n    * Principal \tComponent \tAnalysis(PCA)\n    * Kernel PCA\n    * Locally -Linear\tEmbedding \t(LLE)\n    * t-distributed\tStochastic\tNeighbor\tEmbedding \t(t-SNE)\n    \n* Association \trule\tlearning\n\n    * Apriori\n    * Eclat\n* Semisupervised learning\n* Reinforcement Learning\n    * Q-learning\n* Batch learning & Online learning\n* Ensemble  Learning\n\n**<< Note >>**\n> Here is no method which outperforms all others for all tasks\n\n"
    },
    {
      "metadata": {
        "_uuid": "178ed7d38b21204bf12510521211b971ca291ffa"
      },
      "cell_type": "markdown",
      "source": "## 7-2 Accuracy and precision\nOne of the most important questions to ask as a machine learning engineer when evaluating our model is how to judge our own model?\neach machine learning model is trying to solve a problem with a different objective using a different dataset and hence, it is important to understand the context before choosing a metric.\n<img src='https://cdn-images-1.medium.com/max/1200/1*8VM2PELQ-oeM0O3ya7BIyQ.png' height=600 width=600>\n### 7-2-1 RMSE\nRoot-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n<img src='https://i.stack.imgur.com/eG03B.png' height= 200 width=350>\n \n [go to top](#top)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "74a78a50b810c95c0c72bcba1d89890c9002e527",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "#creating matrices for sklearn:\nX_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]\ny = train.SalePrice",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "09779eedc5526a0f0e180b825aa069a758a61ad4",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "X_train.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "60eadfc578b68b1a71af67ef704e96c86e72507d"
      },
      "cell_type": "markdown",
      "source": "## 7-3 Ridge "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c4b4a3e4c8e9e8a85a314c787dcadb14d18aafb",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "def rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "439962cf50b059393d957bad5eee66dbaac32c84",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "model_ridge = Ridge()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5fbbd633cb8298f95ff07734c96bd1cbda6f5257",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e8d509d57506669573420cb9eb9549c0db2b8bb0"
      },
      "cell_type": "markdown",
      "source": "## 7-3-1 Root Mean Squared Error"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8ed4ca82d0854fdd0b5be4bbb9a7e3e078f0a472",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "cv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d4f4820db8df421a27e446261b576a3cf521a448",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# steps\nsteps = [('scaler', StandardScaler()),\n         ('ridge', Ridge())]\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {'ridge__alpha':np.logspace(-4, 0, 50)}\n\n# Create the GridSearchCV object: cv\ncv = GridSearchCV(pipeline, parameters, cv=3)\n\n# Fit to the training set\ncv.fit(X_train, y)\n\n#predict on train set\ny_pred_train=cv.predict(X_train)\n\n# Predict test set\ny_pred_test=cv.predict(X_test)\n\n# rmse on train set\nrmse = np.sqrt(mean_squared_error(y, y_pred_train))\nprint(\"Root Mean Squared Error: {}\".format(rmse))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0e98e8a4e7e998b67823934f049370ed2b698567"
      },
      "cell_type": "markdown",
      "source": "## 7-3-2 Submission"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f7a972bad31eb6b8c8576980d45e62e76910166b",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# shape to export\noutput=pd.concat([y_id, pd.DataFrame(np.exp(y_pred_test)-1)], axis=1, ignore_index=True)\noutput.columns=['Id', 'SalePrice']\n\n# export\noutput.to_csv('./submission.csv', sep=',', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "97adc471c068fbd8d36ca19a4db0d98b0924c731"
      },
      "cell_type": "markdown",
      "source": "-----------------\n<a id=\"53\"></a> <br>\n## 8- Conclusion\nThis kernel is not completed yet, I will try to cover all the parts related to the process of ML with a variety of Python packages and I know that there are still some problems then I hope to get your feedback to improve it."
    },
    {
      "metadata": {
        "_uuid": "cf3679a51c72dbe2d2549b5fe97e4ac5f1fa0fa0"
      },
      "cell_type": "markdown",
      "source": "You can follow me on:\n<br>\n> ###### [ GitHub](https://github.com/mjbahmani)\n<br>\n--------------------------------------\n\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated** "
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "<a id=\"54\"></a> <br>\n# 9- References\n1. [Https://skymind.ai/wiki/machine-learning-workflow](https://skymind.ai/wiki/machine-learning-workflow)\n1. [Problem-define](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)\n1. [Sklearn](http://scikit-learn.org/)\n1. [Machine-learning-in-python-step-by-step](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)\n1. [Data Cleaning](http://wp.sigmod.org/?p=2288)\n1. [Kaggle kernel](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)\n1. [Choosing-the-right-metric-for-machine-learning-models-part](https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4)\n1. [Unboxing outliers in machine learning](https://medium.com/datadriveninvestor/unboxing-outliers-in-machine-learning-d43fe40d88a6)\n1. [How to handle missing data](https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4)"
    },
    {
      "metadata": {
        "_uuid": "76cc4a1db38546a9c77007f59bd48329b0b8beed"
      },
      "cell_type": "markdown",
      "source": "###   Not Completed Yet !!! "
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}